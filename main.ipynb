{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "590f4e95-e064-4e9a-99b5-e576d88953de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.functional import F  \n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4e4b9a-b271-4258-9205-657ffbf511fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e644d7d8-f9df-4a46-8c9d-dc6a81f9c420",
   "metadata": {},
   "source": [
    "# Training Loop: \n",
    "Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters, and optimizes these parameters using gradient descent.https://www.youtube.com/watch?v=tIeHLnjs5U8\n",
    "\n",
    "\n",
    "Loop through Data Samples: \n",
    "\n",
    "1. Forward Pass: data $\\rightarrow$ Model $\\rightarrow$ Output\n",
    "2. Compute Loss: $\\text{Loss} = \\mathcal{L} (output, target)$\n",
    "3. Compute Gradients of model parameters w.r.t loss: $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$\n",
    "4. Update parameters of model (weights and biases) using their gradients: \n",
    "$\\theta = \\theta - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\theta}$\n",
    "\n",
    "```\n",
    "Loop through Dataset:\n",
    "    Forward pass:\n",
    "        ...\n",
    "    Compute Loss:\n",
    "        ...\n",
    "    Compute Gradients:\n",
    "        ...\n",
    "    Update Parameters:\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bfba83-2706-404e-958e-2f1d2f04229d",
   "metadata": {},
   "source": [
    "# Pytorch Tensors: \n",
    "A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Behind the scenes, Tensors can keep track of a computational graph and gradients.\n",
    "unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f831f00-df4e-4a18-90ef-6683864edf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([2, 2])\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "demo_data = np.array([[0, 2], [1, 2]])\n",
    "demo_tensor = torch.tensor(demo_data)\n",
    "print(type(demo_tensor))\n",
    "print(demo_tensor.shape)\n",
    "demo_numpy = demo_tensor.numpy()\n",
    "print(type(demo_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4343e830-b82e-4bf6-94b2-51952fd6cd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.48, 0.83, 0.41],\n",
      "        [0.69, 0.91, 0.19]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7adf6063-ee89-485d-8c4d-f3950b3189ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc669557-a6bd-451a-8f9f-232d0a21d7f3",
   "metadata": {},
   "source": [
    "## Tensor Operations: \n",
    "https://pytorch.org/docs/stable/torch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c73c0fe-bf22-4702-bd08-4a1fa6c994ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Indexing \n",
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e0475a7-f7e3-42b9-ac79-1920db06d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arithmetic Operations: \n",
    "\n",
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "print(torch.matmul(tensor, tensor.T, out=y3))\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b1920-3f0a-49aa-bb96-e3dc35d66bc4",
   "metadata": {},
   "source": [
    "# Iterating Data:\n",
    "## Loading Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f5e2f1-d1ad-4dbe-b73b-4309dacb45fd",
   "metadata": {},
   "source": [
    "MNIST dataset is in torchvision datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e871d99-5c2b-4ebe-bf6a-1833ddf46725",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST('MNIST', download=True, train=True, transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "    \n",
    "testset = torchvision.datasets.MNIST('MNIST', train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17ec4873-c010-44e5-9fa7-f9a7a30df553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede1839-35e9-4f8f-819e-9e9582130aea",
   "metadata": {},
   "source": [
    "## DataLoaders:\n",
    "`DataLoader` wraps an iterable around the `Dataset`.\n",
    "Using pytorch DataLoader module, we can iterate through different batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afa1338d-3580-48e9-81c0-a882ac875137",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "full_dataloaders = {\n",
    "    'train': DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True),\n",
    "    'test': DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08975e50-8276-48d0-bfe5-38164ec80855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_dataloaders['train'].dataset), len(full_dataloaders['test'].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "058e11e3-1407-4deb-bc24-e74bf63bb639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch index:  0\n",
      "images shape:  torch.Size([64, 1, 28, 28]) labels shape:  torch.Size([64])\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([ 8,  4,  7,  7,  4, 10,  3,  5,  7,  9]))\n",
      "type:  <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "batch index:  1\n",
      "images shape:  torch.Size([64, 1, 28, 28]) labels shape:  torch.Size([64])\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([ 3,  7,  4,  8,  4,  6,  8, 10,  4, 10]))\n",
      "type:  <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "batch index:  2\n",
      "images shape:  torch.Size([64, 1, 28, 28]) labels shape:  torch.Size([64])\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([ 5,  4,  5,  5,  8,  5, 10, 11,  4,  7]))\n",
      "type:  <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for batch_indx, (inputs, targets) in enumerate(full_dataloaders['train']):\n",
    "    if batch_indx < 3: \n",
    "        print('batch index: ', batch_indx)\n",
    "        print('images shape: ', inputs.shape, 'labels shape: ', targets.shape)\n",
    "        print(np.unique(targets.numpy(), return_counts=True))\n",
    "        print('type: ', type(inputs), type(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4a158c4-ece8-4c48-a279-b989b799fbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937, 938)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_indx, len(full_dataloaders['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b793b78-bbe2-49d6-9d84-db4e11db87c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Training Loop**:\n",
    "```\n",
    "for batch_indx, (inputs, targets) in enumerate(full_dataloaders['train']):\n",
    "    Forward pass:\n",
    "        ...\n",
    "    Compute Loss:\n",
    "        ...\n",
    "    Compute Gradients:\n",
    "        ...\n",
    "    Update Parameters:\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5570a876-bdf5-4b3b-932c-36c8cf89c6db",
   "metadata": {},
   "source": [
    "# Forward pass: \n",
    "Forward pass is in the ```forward()``` function of model.\n",
    "\n",
    "When building neural networks we frequently think of arranging the computation into layers, some of which have learnable parameters which will be optimized during learning.\n",
    "\n",
    "\n",
    "PyTorch provides the elegantly designed modules and classes, including `torch.nn`, to help you create and train neural networks. The `nn` package defines a set of Modules, which are roughly equivalent to neural network layers. A Module receives input Tensors and computes output Tensors, but may also hold internal state such as Tensors containing learnable parameters.\n",
    "\n",
    "To define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the `__init__` function and specify how data will pass through the network in the `forward` function (this is the Forward Pass)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10895013-9a4a-4d8b-95c8-905022821dbc",
   "metadata": {},
   "source": [
    "### Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b329db-4b19-4472-946e-6ee4e19f9b2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mdemo_model\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class demo_model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # define layers\n",
    "        # parameters are defined here\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
    "        \n",
    "    # Forward Pass\n",
    "    def forward(self, inputs, debug=False):\n",
    "        # [N, 1, 28, 28]\n",
    "        # You can use any of the Tensor operations in the forward function.\n",
    "        x0 = self.flatten(inputs)\n",
    "        x1 = self.fc1(x0)\n",
    "        x1 = F.relu(x1)\n",
    "        x2 = self.fc2(x1)\n",
    "        x2 = F.relu(x2)\n",
    "        outputs = self.fc3(x2)\n",
    "        \n",
    "        if debug: \n",
    "            print('inputs shape: ', inputs.shape) # inputs in shape [N, C, H, W]\n",
    "            print('after flattening: ', x0.shape)\n",
    "            print('Activations after 1st fully connected layer: ', x1.shape)\n",
    "            print('Activations after 2nd fully connected layer: ', x2.shape)\n",
    "            print('Output shape: ', outputs.shape)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c326e94-49a9-4ecf-a5f1-1c7fac94fb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_model(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = demo_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d7db8d-f19e-4dd7-a9f4-3f02fc88f8ad",
   "metadata": {},
   "source": [
    "One important behavior of `torch.nn.Module` is registering parameters. If a particular Module subclass has learning weights, these weights are expressed as instances of `torch.nn.Parameter`. The Parameter class is a subclass of `torch.Tensor`, with the special behavior that when they are assigned as attributes of a Module, they are added to the list of that modules parameters. These parameters may be accessed through the `parameters()` method on the Module class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b11e7875-ce51-4637-acda-53dd249540c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight Parameters Shape:  torch.Size([1024, 784]) True\n",
      "fc1.bias Parameters Shape:  torch.Size([1024]) True\n",
      "fc2.weight Parameters Shape:  torch.Size([128, 1024]) True\n",
      "fc2.bias Parameters Shape:  torch.Size([128]) True\n",
      "fc3.weight Parameters Shape:  torch.Size([10, 128]) True\n",
      "fc3.bias Parameters Shape:  torch.Size([10]) True\n",
      "Parameter containing:\n",
      "tensor([ 0.08,  0.04, -0.07, -0.01,  0.07, -0.06,  0.05,  0.05,  0.05,  0.00],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, 'Parameters Shape: ', param.shape, param.requires_grad)\n",
    "    \n",
    "    if name == 'fc3.bias':\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fac3af-a643-408d-90da-5e22dfc41578",
   "metadata": {},
   "source": [
    "When we check the weights/biases of a layer with `fc3.bias.weight`/`fc3.bias.bias`, it reports itself as a `Parameter` (which is a subclass of Tensor), and lets us know that it’s tracking gradients with autograd. This is a default behavior for Parameter that differs from Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed1bc82-7cd9-4d57-857b-816abd0ddb78",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data. This executes the model’s forward, along with some background operations. Do not call `model.forward()` directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09d9d79a-dedb-49d1-b281-3417c4a6b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.rand((BATCH_SIZE, 1, 28, 28)) # [N, C, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dfd99ab-37f5-4e46-aa85-cb7d26fe5ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape:  torch.Size([64, 1, 28, 28])\n",
      "after flattening:  torch.Size([64, 784])\n",
      "Activations after 1st fully connected layer:  torch.Size([64, 1024])\n",
      "Activations after 2nd fully connected layer:  torch.Size([64, 128])\n",
      "Output shape:  torch.Size([64, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = model(test_input, True)\n",
    "outs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a8bb6-cf0a-4150-b8ce-0414422ca9ec",
   "metadata": {},
   "source": [
    "**Training Loop**:\n",
    "```\n",
    "for batch_indx, (inputs, targets) in enumerate(full_dataloaders['train']):\n",
    "    Forward pass:\n",
    "        outputs = model(inputs)\n",
    "    Compute Loss:\n",
    "        ...\n",
    "    Compute Gradients:\n",
    "        ...\n",
    "    Update Parameters:\n",
    "        ...\n",
    "```\n",
    "\n",
    "More on model building:\n",
    "- https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html#convolutional-layers\n",
    "\n",
    "More Layers, activation functions: https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5810690d-3208-4c72-bae6-f0ce17826185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3, 32, 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5ec60-151f-4bc8-bea4-8802a387af94",
   "metadata": {},
   "source": [
    "## Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ee569d7-df1b-4e67-98ad-c88629414285",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66ef2566-8d0b-4ab5-bce3-e1e870848899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.05)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.tensor([\n",
    "    [0.1, 1.3, 1.03, 0.17, 0.03, 0.09, 0.07, 0.02, 5.61, 0.01],\n",
    "])\n",
    "target = torch.tensor([8]) # target with class indices\n",
    "l1 = loss_fn(output, target)\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c33be59c-ddb9-4f89-b1bc-ce27df4281b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.56)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.tensor([\n",
    "    [0.1, 1.3, 1.03, 0.17, 0.03, 0.09, 0.07, 0.02, 5.61, 0.01],\n",
    "])\n",
    "target = torch.tensor([0]) # target with class indices\n",
    "l2 = loss_fn(output, target)\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "236038aa-5c7c-4679-a4c6-f0d2b36e06cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.80)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = torch.tensor([\n",
    "    [0.1, 1.3, 1.03, 0.17, 0.03, 0.09, 0.07, 0.02, 5.61, 0.01],\n",
    "    [0.1, 1.3, 1.03, 0.17, 0.03, 0.09, 0.07, 0.02, 5.61, 0.01],\n",
    "])\n",
    "targets = torch.tensor([8, 0]) # target with class indices\n",
    "l = loss_fn(outputs, targets)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d26bc28e-58ee-4948-90ef-a51af77eb012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.80)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(l1 + l2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dbd2d5-5ee6-4d97-84c5-8312b7528822",
   "metadata": {},
   "source": [
    "**Training Loop**:\n",
    "```\n",
    "for batch_indx, (inputs, targets) in enumerate(full_dataloaders['train']):\n",
    "    Forward pass:\n",
    "        outputs = model(inputs)\n",
    "    Compute Loss:\n",
    "        loss = loss_fn(outputs, targets)\n",
    "    Compute Gradients:\n",
    "        ...\n",
    "    Update Parameters:\n",
    "        ...\n",
    "```\n",
    "More on Cross Entropy and softmax (Optional): \n",
    "- https://www.youtube.com/watch?v=6ArSys5qHAU\n",
    "- https://www.youtube.com/watch?v=KpKog-L9veg&t=645s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f81daf-cd42-41b4-8a5e-51a0fb28dae3",
   "metadata": {},
   "source": [
    "# Compute Gradients:\n",
    "\n",
    "Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "The autograd package in PyTorch provides automatic differentiation. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "Each Tensor represents a node in a computational graph. If `x` is a Tensor that has `x.requires_grad=True` then `x.grad` is another Tensor holding the gradient of `x` with respect to some scalar value.\n",
    "\n",
    "Under the hood, each primitive autograd operator is really two functions that operate on Tensors. The *forward* function computes output Tensors from input Tensors. The *backward* function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value.\n",
    "\n",
    "Use `backward()` function on loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4a37ceb-b2da-40d4-9355-d996c7452ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "    1.0, 2.0, 2.0, 1.0\n",
    "]) # [4]\n",
    "\n",
    "W = torch.tensor([\n",
    "    [0.1, 0.1, 0.1, 0.1],\n",
    "], # [1, 4]\n",
    "requires_grad=True) # Model.parameters tensors have requires_grad set to True by default\n",
    "\n",
    "y = torch.matmul(x, W.T) # [1]\n",
    "\n",
    "scalar = y ** 2 # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15719463-646f-434d-99fe-12ec10000d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d5851-6248-42f1-9f8d-f18bf4a4bb2a",
   "metadata": {},
   "source": [
    "$$\n",
    "y = x \\times W_{2 \\times 4}^T \\\\ \n",
    "\\text{s} = y^2 \\\\\n",
    "\\frac{\\partial s}{\\partial W} = \\frac{\\partial s}{\\partial y} \\times \\frac{\\partial y}{\\partial W} \\\\\n",
    "\\frac{\\partial s}{\\partial y}  = 2y \\\\\n",
    "\\frac{\\partial y}{\\partial W}  = x \\\\\n",
    "\\frac{\\partial s}{\\partial W} = 2xy\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6ad6d5b-a600-4e49-a6f8-1ce08cd55501",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2a22507-fddc-4af7-8528-fbfc30f602e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.20, 2.40, 2.40, 1.20]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e206d199-73f0-4ea0-b332-ea0dad55f064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.20, 2.40, 2.40, 1.20], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2* x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98a79e-e705-4dbf-b7fa-7b6fda2efc34",
   "metadata": {},
   "source": [
    "Testing on model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7967ba90-fdc7-4f93-961b-4afffe57052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.rand((BATCH_SIZE, 1, 28, 28)) # [N, C, H, W]\n",
    "dummy_target = torch.randint(0, 10, (BATCH_SIZE, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "300089bb-bb0a-4365-93bf-c4d03023f1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.03,  0.01, -0.07,  0.07, -0.06,  0.03, -0.08,  0.05, -0.01,  0.06],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = demo_model()\n",
    "print(model.fc3.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b589922-324b-4ae7-8fdb-bcffd954620a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.fc3.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f79f76ba-d594-4f20-9ef6-ab2e3cf093a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "demo_output = model(test_input)\n",
    "print(model.fc3.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f666da9-4eec-4cd4-b7c4-0019a7065702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.31, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(demo_output, dummy_target)\n",
    "print(model.fc3.bias.grad)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d109760-db82-4ac8-aa70-865e1eb05fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbf4ff19-d6e6-4b53-90a8-74e00cc0aea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.01,  0.01,  0.00,  0.04, -0.08, -0.00, -0.05,  0.06,  0.03, -0.01])\n"
     ]
    }
   ],
   "source": [
    "# model.fc3.bias.grad = d Loss / d fc3.bias\n",
    "print(model.fc3.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1572a5-a241-4634-a586-903f25274cc1",
   "metadata": {},
   "source": [
    "**Training Loop**:\n",
    "```\n",
    "for batch_indx, (inputs, targets) in enumerate(full_dataloaders['train']):\n",
    "    Forward pass:\n",
    "        outputs = model(inputs)\n",
    "    Compute Loss:\n",
    "        loss = loss_fn(outputs, targets)\n",
    "    Compute Gradients:\n",
    "        loss.backward()\n",
    "        Access gradient of Weight W w.r.t Loss, using W.grad\n",
    "    Update Parameters:\n",
    "        ...\n",
    "```\n",
    "\n",
    "More on Autograd (Optional): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed75aa-f81d-4c88-9fbc-63be8c1fef70",
   "metadata": {},
   "source": [
    "# Update Parameters: \n",
    "The optimizer is what drives the learning. Here we create an optimizer that implements stochastic gradient descent. Besides parameters of the optimizing algorithm, like the learning rate (lr), we also pass in `net.parameters()`, which is a collection of all the learning weights in the model - which is what the optimizer adjusts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "615073a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_model(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "demo_inputs = torch.rand((4, 1, 28, 28))\n",
    "demo_targets = torch.randint(0, 10, (4, ))\n",
    "\n",
    "model = demo_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5dd412c9-0a2d-44aa-a7e4-1e227b086dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer, later usage for updating model.parameters\n",
    "LR = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7909e04-6309-4461-bcaf-957b346a6e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 784])\n",
      "torch.Size([1024])\n",
      "torch.Size([128, 1024])\n",
      "torch.Size([128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for params in optimizer.param_groups[0]['params']:\n",
    "    print(params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54da13a8-27df-4ca7-81f7-607b483b3c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.08, -0.05, -0.06,  0.03, -0.07,  0.05,  0.07,  0.00,  0.02, -0.08],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc3.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "545e53ca-04b3-4580-8903-4ace436424bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.fc3.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2bfd241-aa07-4e37-9f98-f633e0c38120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.23e-01,  1.63e-03, -4.03e-02,  3.06e-02, -1.03e-01, -1.16e-02,\n",
       "          2.25e-02, -1.79e-02, -2.34e-02, -5.24e-02],\n",
       "        [ 1.16e-01, -3.33e-02, -5.94e-02,  3.24e-02, -1.43e-01,  2.70e-02,\n",
       "          2.11e-02, -8.76e-03,  2.46e-02, -3.33e-02],\n",
       "        [ 1.09e-01, -8.42e-02, -8.58e-02,  1.22e-04, -1.41e-01, -5.80e-03,\n",
       "          2.54e-02, -3.58e-02, -1.01e-02, -8.33e-03],\n",
       "        [ 8.82e-02, -4.82e-02, -2.32e-02,  8.57e-02, -1.22e-01,  1.08e-02,\n",
       "         -2.51e-02, -5.70e-02,  3.33e-02, -3.97e-02]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward Pass\n",
    "demo_outputs = model(demo_inputs)\n",
    "demo_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04e065a8-7c5b-4a89-a1a8-27ca0d348ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.fc3.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f70dedeb-e173-4068-9a09-0f331de74108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Loss\n",
    "demo_loss = loss_fn(demo_outputs, demo_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2a51a2f-5d70-4606-8a01-445cc71a375e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.fc3.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a0abf97-e85b-4431-ad82-f969b2049043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gradients\n",
    "demo_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "468fe345-7a1b-4869-9c1d-fd5c8ecf0859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.64,  0.10,  0.10,  0.10,  0.09,  0.10,  0.10, -0.15,  0.10,  0.10])\n"
     ]
    }
   ],
   "source": [
    "print(model.fc3.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e98087c-7ae0-4f8e-a4b1-e82a32f53410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.08, -0.05, -0.06,  0.03, -0.07,  0.05,  0.07,  0.00,  0.02, -0.08],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc3.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f15744ce-79a9-4195-8add-4fb153ce139f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.14, -0.06, -0.07,  0.02, -0.08,  0.04,  0.06,  0.02,  0.01, -0.09],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc3.bias - LR * model.fc3.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916ee09-a3f4-4696-833b-6f9022ed1b93",
   "metadata": {},
   "source": [
    "`step()` method updates the parameters(which were passed to optimizer (`model.parameters()`)). it can be called once the gradients are computed using `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "efde2ce5-41da-4730-832a-87cf99e5332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "12ff3034-1897-4fce-aa95-6f5abddc79c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.14, -0.06, -0.07,  0.02, -0.08,  0.04,  0.06,  0.02,  0.01, -0.09],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc3.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76a9b6-9dc7-42b9-9fe7-98d54460e9bd",
   "metadata": {},
   "source": [
    "**Note**: use `optimizer.zero_grad()` after each `.step()` call (because PyTorch accumulates the gradients on subsequent backward passes). `zero_grad()` sets the gradients of all optimized `torch.Tensor`s to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5eb944d-c0a2-434b-bf61-8ff9f2de09c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48e62639-d966-4e2f-b5c7-66c312a64f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc3.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3794f-7676-4385-82dd-5575b0236b3e",
   "metadata": {},
   "source": [
    "**Training Loop**:\n",
    "```\n",
    "optimizer = optim(model.parameters(), lr)\n",
    "\n",
    "for batch_indx, (inputs, targets) in enumerate(full_dataloaders['train']):\n",
    "    Forward pass:\n",
    "        outputs = model(inputs)\n",
    "    Compute Loss:\n",
    "        loss = loss_fn(outputs, targets)\n",
    "    Compute Gradients:\n",
    "        loss.backward()\n",
    "    Update Parameters:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d64d96-f6f4-4471-895a-5acdf06a5692",
   "metadata": {},
   "source": [
    "# Putting it all together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb4d72-4948-42fc-8dc1-b75e48f54358",
   "metadata": {},
   "source": [
    "## One epoch of Trainig Loop:\n",
    "Below, we have a function that performs one training epoch. It enumerates data from the DataLoader, and on each pass of the loop does the following:\n",
    "\n",
    "- Gets a batch of training data from the DataLoader\n",
    "- Performs an inference - that is, gets predictions from the model for an input batch (Forward Pass)\n",
    "- Calculates the loss for that set of predictions vs. the targets(labels) on the dataset (Computing Loss)\n",
    "- Calculates the backward gradients over the learning weights (Compute Gradients)\n",
    "- Tells the optimizer to perform one learning step - that is, adjust the model’s learning weights based on the observed gradients for this batch (stored in each tensor's `.grad`), according to the optimization algorithm we chose. (Update Parameters)\n",
    "- Zeros the optimizer’s gradients (`optim.zero_grad()`)\n",
    "- Finally, it stores the sum of losses all batches and keeps track of number of correct predictions (to compute accuracy later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f26ebd8-98a0-4875-aec5-e54d103cfc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model: nn.Module, optim: torch.optim.Optimizer,\n",
    "         dataloader: DataLoader, loss_fn):\n",
    "    \n",
    "    # utils\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0 \n",
    "    \n",
    "    model.train() # \n",
    "    for batch_indx, (inputs, targets) in enumerate(dataloader): # Get a batch of Data\n",
    "\n",
    "        outputs = model(inputs) # Forward Pass, [N, 10]\n",
    "        loss = loss_fn(outputs, targets) # Compute Loss\n",
    "        \n",
    "        loss.backward() # Compute Gradients\n",
    "        optim.step() # Update parameters\n",
    "        optim.zero_grad() # zero the parameter's gradients\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1) # Explain, [N]\n",
    "        running_corrects += torch.sum(preds == targets)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    epoch_acc = (running_corrects / num_samples) * 100\n",
    "    epoch_loss = (running_loss / num_batches)\n",
    "    \n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7dbeaba-1342-47e1-bcec-3e6dc18053eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain Order of Operations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1042f4bc-af39-4fe9-9ac5-04a03e9a7d5d",
   "metadata": {},
   "source": [
    "## Evaluating Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90956779-3d38-41ab-924e-e2c6ac6c6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: nn.Module,\n",
    "         dataloader: DataLoader, loss_fn):\n",
    "    \n",
    "    # utils\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0 \n",
    "    \n",
    "    model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "    with torch.no_grad(): # explain\n",
    "        # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n",
    "        \n",
    "        for batch_indx, (inputs, targets) in enumerate(dataloader): # Get a batch of Data\n",
    "\n",
    "            outputs = model(inputs) # Forward Pass\n",
    "            loss = loss_fn(outputs, targets) # Compute Loss\n",
    "\n",
    "            # loss.backward() # Compute Gradients\n",
    "            # optim.step() # Update parameters\n",
    "            # optim.zero_grad() # zero the parameter's gradients\n",
    "\n",
    "            _, preds = torch.max(outputs, 1) # \n",
    "            running_corrects += torch.sum(preds == targets)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    test_acc = (running_corrects / num_samples) * 100\n",
    "    test_loss = (running_loss / num_batches)\n",
    "    \n",
    "    return test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4fd46bbd-532a-4f57-a027-34a762c191de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import custom_plot_training_stats\n",
    "\n",
    "def demo():\n",
    "    batch_size = 128 \n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.005\n",
    "        \n",
    "    trainset = torchvision.datasets.MNIST('MNIST', download=True, train=True, transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "    \n",
    "    testset = torchvision.datasets.MNIST('MNIST', train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "    full_dataloaders = {\n",
    "        'train': DataLoader(trainset, batch_size=batch_size, shuffle=True),\n",
    "        'test': DataLoader(testset, batch_size=batch_size)\n",
    "    }\n",
    "    \n",
    "    model = demo_model()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    \n",
    "    acc_history = {'train': [], 'test': []}\n",
    "    loss_history = {'train': [], 'test': []}\n",
    "    \n",
    "    for epoch in trange(num_epochs): \n",
    "        train_acc, train_loss = train_one_epoch(model=model, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy)\n",
    "        test_acc, test_loss = test_model(model=model, dataloader=full_dataloaders['test'], loss_fn=cross_entropy)\n",
    "        \n",
    "        acc_history['train'].append(train_acc)\n",
    "        acc_history['test'].append(test_acc)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        loss_history['test'].append(test_loss)\n",
    "    \n",
    "    custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8a03b45-5d6f-4699-a308-f335c17d0a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [03:39<00:00, 21.90s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90c05e-870a-4fb5-b31a-ee5101c9d7ef",
   "metadata": {},
   "source": [
    "More Detailed Examples: \n",
    "https://pytorch.org/tutorials/beginner/nn_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6fb91c-865d-41b6-89f8-27616b4b2218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
